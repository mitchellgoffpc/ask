import os
import time
import torch
import argparse
import numpy as np
import gymnasium as gym
import multiprocessing as mp
from tqdm import tqdm
from pathlib import Path
from cartpole.models import MLP

def run_simulation(env, action):
    done = False
    trunc = False
    total_reward = 0
    steps = 0

    while not done and not trunc:
        observation, reward, done, trunc, _ = env.step(action)
        total_reward += reward
        steps += 1
        action = env.action_space.sample()  # random action for subsequent steps

    return steps

def search_best_action(env, num_simulations):
    save_state_env = gym.make('CartPole-v1')
    actions = list(range(env.action_space.n))
    action_results = {i: [] for i in actions}

    for action in actions:
        for _ in range(num_simulations):
            save_state_env.reset()
            save_state_env.unwrapped.state = env.unwrapped.state
            steps = run_simulation(save_state_env, action)
            action_results[action].append(steps)

    action_results = {k: np.mean(v) for k,v in action_results.items()}
    best_action = max(action_results, key=action_results.get)
    return best_action, action_results

def run_episode(model_path, num_simulations, epsilon, render=False, save=False):
    model = None
    if model_path:
        model = MLP(input_size=4, output_size=2, hidden_size=64).eval()
        model.load_state_dict(torch.load(model_path))

    env = gym.make('CartPole-v1', render_mode='human' if render else None)
    observation, _ = env.reset()
    env.step(env.action_space.sample())  # state is a numpy array on the first iter, after the first step it becomes a tuple

    done = False
    trunc = False
    total_reward = 1
    steps = 1
    st = time.time()
    episode_data = []

    while not done and not trunc:
        tree_decision = None
        if save or not model:  # only compute the tree decision if we have to
            tree_decision, _ = search_best_action(env, num_simulations)

        if np.random.uniform() < epsilon:
            action = env.action_space.sample()
        elif model:
            with torch.no_grad():
                # action = torch.argmax(model(torch.FloatTensor(observation))).item()
                action_probs = torch.softmax(model(torch.FloatTensor(observation)), dim=0)
                action = torch.multinomial(action_probs, 1).item()
        else:
            action = tree_decision

        observation, reward, done, trunc, _ = env.step(action)
        total_reward += reward
        steps += 1
        episode_data.append({'step': steps, 'observation': observation, 'action': action, 'tree_decision': tree_decision})

    env.close()
    et = time.time()
    observations, actions, tree_decisions = zip(*[(data['observation'], data['action'], data['tree_decision']) for data in episode_data])

    return steps, observations, actions, tree_decisions, et - st

def _run_episode(args):
    return run_episode(*args)

def run_multiple_episodes(model_path, episode_dir, num_episodes, num_simulations, epsilon):
    st = time.time()
    with mp.Pool(processes=mp.cpu_count()) as pool:
        results = list(tqdm(pool.imap(_run_episode, [(model_path, num_simulations, epsilon, False, True) for i in range(num_episodes)]), total=num_episodes))
    et = time.time()

    if episode_dir is not None:
        for i, (_, observations, actions, tree_decisions, _) in enumerate(results):
            np.savez(episode_dir / f'episode_{i:03d}.npz', observations=observations, actions=actions, tree_decisions=tree_decisions)

    steps, _, _, _, times = zip(*results)
    print(f"Results after {num_episodes} episodes:")
    print(f"Avg Steps: {np.mean(steps):.2f}")
    print(f"Avg Time: {np.mean(times):.2f} seconds")
    print(f"Total time: {et - st:.2f} seconds")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Run CartPole episodes with optional model.')
    parser.add_argument('-m', '--model', type=str, help='Path to the policy model checkpoint')
    parser.add_argument('-n', '--num_episodes', type=int, default=100, help='Number of episodes to run')
    parser.add_argument('-s', '--num_simulations', type=int, default=50, help='Number of simulations to run')
    parser.add_argument('-e', '--epsilon', type=float, default=0.0, help='Epsilon value for epsilon-greedy policy')
    parser.add_argument('-o', '--output', help='Save the episodes to disk')
    parser.add_argument('--render', action='store_true', help='Render the episodes')
    args = parser.parse_args()

    episode_dir = None
    if args.output:
        episode_dir = Path(args.output)
        episode_dir.mkdir(parents=True, exist_ok=True)
        for f in episode_dir.glob('episode_*.npz'):
            f.unlink()

    if args.render:
        steps, *_ = run_episode(args.model, args.num_simulations, args.epsilon, render=True)
        print(f"Episode finished after {steps} steps.")
    else:
        run_multiple_episodes(args.model, episode_dir, args.num_episodes, args.num_simulations, args.epsilon)
