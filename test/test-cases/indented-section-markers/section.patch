#!/usr/bin/env python
import os
import time
import types
import torch
import argparse
import requests
import tiktoken
import safetensors
from pathlib import Path
from tqdm import tqdm, trange
from tiktoken.load import load_tiktoken_bpe
from models.gpt import GPT, GPTConfig
from models.llama import Llama, LlamaConfig

[UNCHANGED]

def load_file(url, output_fn):
    tmp_output_fn = f'{output_fn}.tmp'
    if not os.path.exists(output_fn):
        headers = {'Authorization': f'Bearer {HUGGINGFACE_API_KEY}'} if HUGGINGFACE_API_KEY else {}
        r = requests.get(url, headers=headers, stream=True)
        r.raise_for_status()
        file_size = int(r.headers['content-length'])
        chunk_size = 128 * 1000
        [UNCHANGED]

def load_checkpoint(weights_url, checkpoint_fn):
    [UNCHANGED]

def load_checkpoints(weights_urls, checkpoint_fns):
    [UNCHANGED]

def load_llama_tokenizer(model_url, model_fn):
    [UNCHANGED]

def fix_gpt_state_dict(state_dict):
    replacements = {
        'h.': 'blocks.',
        'wte.': 'embed_tokens.',
        'wpe.': 'embed_pos.',
        'attn.c_attn': 'attn.qkv',
        'attn.c_proj': 'attn.proj',
        'mlp.c_fc': 'mlp.fc1',
        'mlp.c_proj': 'mlp.fc2',
        'ln_1.': 'ln1.',
        'ln_2.': 'ln2.',
        'ln_f.': 'norm.'}
    linears = ['attn.qkv.weight', 'attn.proj.weight', 'mlp.fc1.weight', 'mlp.fc2.weight']
    biases = ['attn.bias', 'attn.masked_bias']

    for src, dst in replacements.items():
        state_dict = {k.replace(src, dst): v for k,v in state_dict.items()}
    state_dict = {k:v for k,v in state_dict.items() if not any(x in k for x in biases)}
    state_dict = {k: v.transpose(-1, -2) if any(x in k for x in linears) else v for k,v in state_dict.items()}
    state_dict['fc_out.weight'] = state_dict['embed_tokens.weight']
    return state_dict

def fix_llama_state_dict(state_dict):
    replacements = {
        'model.': '',
        'layers.': 'blocks.',
        'input_layernorm': 'ln1',
        'post_attention_layernorm': 'ln2',
        'self_attn.': 'attn.',
        'attn.o_proj.': 'attn.out_proj.',
        'mlp.': 'ff.',
        'lm_head.': 'out_proj.'}
    for src, dst in replacements.items():
        state_dict = {k.replace(src, dst): v for k,v in state_dict.items()}
    return state_dict

[UNCHANGED]